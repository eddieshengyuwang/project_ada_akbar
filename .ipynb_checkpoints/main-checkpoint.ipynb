{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages to perform the data analysis\n",
    "\n",
    "# packages to process and visualize the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import othe useful packages\n",
    "import os\n",
    "\n",
    "# packages to improve visual description and analysis\n",
    "from IPython.core.display import display\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting file names\n",
    "\n",
    "DATA_FOLDER = './data/'\n",
    "\n",
    "# Panama papers data\n",
    "edges_csv = 'panama_papers.edges.csv'\n",
    "intermediary_csv = 'panama_papers.nodes.intermediary.csv' # company or individuals\n",
    "address_csv = 'panama_papers.nodes.address.csv'\n",
    "officer_csv = 'panama_papers.nodes.officer.csv'\n",
    "entity_csv = 'panama_papers.nodes.entity.csv' # tax heaven companies\n",
    "\n",
    "# forbes data\n",
    "forbes_csv = 'Forbes2015.csv'\n",
    "\n",
    "#World Bank dataset\n",
    "GINI_coeff = 'API_SI.POV.GINI_DS2_en_csv_v2_10224868.csv'\n",
    "ease_of_bus = 'API_IC.BUS.EASE.XQ_DS2_en_csv_v2_10226725.csv'\n",
    "GDP_tot = 'API_NY.GDP.MKTP.CD_DS2_en_csv_v2_10224782.csv'\n",
    "GDP_cap = 'API_NY.GDP.PCAP.CD_DS2_en_csv_v2_10224851.csv'\n",
    "time_start_bus = 'API_IC.REG.DURS_DS2_en_csv_v2_10225592.csv'\n",
    "time_spent_by_bus = 'API_IC.GOV.DURS.ZS_DS2_en_csv_v2_10230883.csv'\n",
    "tax_rate = 'API_IC.TAX.TOTL.CP.ZS_DS2_en_csv_v2_10226097.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Panama Papers data into DataFrames\n",
    "# edges = pd.read_csv(DATA_FOLDER + edges_csv) # mixed columns\n",
    "# intermediary = pd.read_csv(DATA_FOLDER + intermediary_csv)\n",
    "# address = pd.read_csv(DATA_FOLDER + address_csv)\n",
    "# officer = pd.read_csv(DATA_FOLDER + officer_csv)\n",
    "# entity = pd.read_csv(DATA_FOLDER + entity_csv) # mixed columns\n",
    "df_edges_raw = pd.read_csv(DATA_FOLDER + edges_csv,low_memory=False)\n",
    "df_address_raw = pd.read_csv(DATA_FOLDER + address_csv,low_memory=False)\n",
    "df_entity_raw = pd.read_csv(DATA_FOLDER + entity_csv,low_memory=False)\n",
    "df_intermediary_raw = pd.read_csv(DATA_FOLDER + intermediary_csv,low_memory=False)\n",
    "df_officier_raw = pd.read_csv(DATA_FOLDER + officer_csv,low_memory=False)\n",
    "\n",
    "# Importing forbes data\n",
    "#dF_forbes_2000 = pd.read_csv(DATA_FOLDER + forbes_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing World Bank Data\n",
    "df_GDP_tot_raw=pd.read_csv(path + GDP_tot,skiprows=4)\n",
    "df_GDP_cap_raw=pd.read_csv(path + GDP_cap,skiprows=4)\n",
    "df_tax_weight=pd.read_csv(path+tax_rate, skiprows=4)\n",
    "df_ease_business=pd.read_csv(path+ease_of_bus,skiprows=4)\n",
    "df_gini=pd.read_csv(path+GINI_coeff, skiprows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling data size\n",
    "\n",
    "First we will answer the question of whether we can handle the data in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing shapes of each DataFrame to see if any are too large\n",
    "\n",
    "print(\"Edges shape: \", df_edges_raw.shape)\n",
    "print(\"Intermediary shape: \", df_intermediary_raw.shape)\n",
    "print(\"Address shape: \", df_address_raw.shape)\n",
    "print(\"Officer shape: \", df_officier_raw.shape)\n",
    "print(\"Entity shape: \", df_entity_raw.shape)\n",
    "\n",
    "print(\"Forbes_2000 shape: \", forbes_2000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we see that the DataFrame with the most number of rows is around 670,000 rows of data. In total the size of the Panama Papers dataset is around 352 MB, which is small enough for Jupyter Notebook and Pandas to handle effectively. Thus the data can definitely be handled within our approach of using purely a notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "\n",
    "Next we understand the data. Although the dataset is downloaded within `csv` format, the data is curated to work well within a graph database (specifically Neo4j as mentioned on the Panama Papers website). Here, the `Edges` table represents the edge connections within a graph database whereas all the other tables (`Intermediary`, `Address`, etc) are nodes within the graph. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_edges_raw.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret this row in the `Edges` table, the `START_ID` and `END_ID` represent the node ids that the edge is connected to. But the node ids can correspond to any of `Intermediary, Address, Officer, Entity`, which may prove to be difficult in SQL or Pandas merging as we will have to look up the node id in every other table. One solution would be to use Neo4j and the Cypher querying language, which can be used to easily interpret the above information. Below we can see clearly that the start node is an Entity named *CARPENTER NELSON & CO., LTD* and the end node is an Address named *EUROFIN SERVICES S.A. P.O.BOX  6003 LAUSANNE 1002, VAUD SWITZERLAND* and the relationship type is *registered address*.\n",
    "\n",
    "<img src=\"ex_query1.png\" height=\"400\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of our project, we may not even need to use Neo4j to answer our queries. However we have done the above analyis to identify the format of our data, which is crucial because if we do require a certain query, we understand how we can leverage different tools to achieve our goals.\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "We see that `Edges` Dataframe and `Entity` Dataframe have mixed datatypes in their columns so we will inspect those more clearly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates = df_edges_raw.start_date.unique()[1:]\n",
    "start_dates = sorted(list(map(lambda x: int(x[-4:]), start_dates)), reverse=True)\n",
    "print(start_dates[:20])\n",
    "print(start_dates[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first see that the start_date and end_date columns in `Edges` have dates that are nonsensical, such as years that are well before/after 2015. Upon consideration of our research questions, we decide to drop the date columns from `Edges` as well as the Note column in `Entity` (For our research question of timeline of leaked papers, we will need to use the `Entity['incorporation_date']` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = df_edges_raw.drop(columns=['start_date', 'end_date'])\n",
    "entity = df_entity_raw.drop(columns=['note'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we see that the `Entity['incorporation_date']` column have acceptable ranges below so we will convert that column from type object to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entity['incorporation_date'].sort_values().unique())\n",
    "entity.incorporation_date = pd.to_datetime(entity.incorporation_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will add country codes in the `Forbes` dataset so that countries can be easily matched between that dataset with the Panana papers data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "forbes_2000.loc[forbes_2000.Country == \"Russia\", 'Country'] = \"Russian Federation\"\n",
    "forbes_2000.loc[forbes_2000.Country == \"South Korea\", 'Country'] = 'Korea, Republic of'\n",
    "forbes_2000['Country Code'] = forbes_2000.apply(lambda row: pycountry.countries.lookup(row['Country']).alpha_3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sociological Research\n",
    "\n",
    "**1)** Map the tax heavens jurisdiction in terms of the number of entities they have registered \n",
    "\n",
    "**2)** Map countries in the world according to how much they appear in the Panama Papers in terms of the entities\n",
    "\n",
    "**3)** Explore the GINI coefficient which is a measure of inequality: what is the correlation between counts and gini?\n",
    "\n",
    "**4)** Explore the HDI coefficient which is a measure of development: what is the correlation between counts and HDI?\n",
    "\n",
    "**5)** Explore the indices of informal sector, organized crime coefficient which is a measure of inequality: what is the correlation between counts and these indicators?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economical Research\n",
    "\n",
    "**1)** Find how many companies in Fortune 2000 are involved in Panama Papers, present the top rank in the result. NOT FEASIBLE\n",
    "\n",
    "**2)** Timeline of number of leaked papers over the years (graph). Try to extrapolate a tendecy in this phenomena.\n",
    "\n",
    "**3)** Amount of lost capital for a country due to tax evasion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the second question, we will look at the incorporation dates of different entities and groupby count by year to plot a time series chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity['Year'] = entity.apply(lambda row: row['incorporation_date'].year, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = entity.groupby(['Year'], axis=0)['Year'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = series.index.tolist()\n",
    "y = series.tolist()\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we observe that the number of incorporations peaked between 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
